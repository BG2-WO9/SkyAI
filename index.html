<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Локальный ИИ в браузере</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      max-width: 800px;
      margin: 20px auto;
      padding: 20px;
      background-color: #f5f9fc;
    }
    h1 {
      text-align: center;
      color: #2c3e50;
    }
    #chat {
      border: 1px solid #ddd;
      height: 400px;
      overflow-y: auto;
      padding: 15px;
      margin: 20px 0;
      background: white;
      border-radius: 8px;
    }
    #userInput {
      width: 70%;
      padding: 12px;
      border: 1px solid #ccc;
      border-radius: 6px;
      font-size: 16px;
    }
    button {
      width: 25%;
      padding: 12px;
      background: #3498db;
      color: white;
      border: none;
      border-radius: 6px;
      cursor: pointer;
      font-size: 16px;
    }
    button:hover {
      background: #2980b9;
    }
    .user {
      color: #2c3e50;
      font-weight: 600;
    }
    .ai {
      color: #16a085;
      font-style: italic;
    }
    .loading {
      color: #7f8c8d;
      font-style: italic;
    }
  </style>
</head>
<body>
  <h1>Локальный ИИ (без интернета)</h1>
  <div id="chat"></div>
  <input type="text" id="userInput" placeholder="Задайте вопрос...">
  <button onclick="sendMessage()">Отправить</button>

  <!-- Библиотека llama.cpp для браузера (WebAssembly) -->
  <script src="https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.0/dist/web-llm.js"></script>


  <script>
    // Конфигурация модели (выбираем лёгкую для браузера)
    const config = {
      model_type: "Llama-3-8B-Instruct-q4f16_1", // Квантованная версия
      gpu_enabled: false, // Отключаем GPU для совместимости
      progress_callback: (progress) => {
        console.log(`Загрузка: ${Math.round(progress * 100)}%`);
      }
    };

    let llama = null;

    // Инициализация ИИ при загрузке страницы
    async function initAI() {
      try {
        llama = await webllm.create(config);
        addMessage("ИИ готов! Задайте любой вопрос.", "ai");
      } catch (error) {
        addMessage("Ошибка загрузки модели: " + error.message, "ai");
      }
    }

    // Отправка сообщения
    async function sendMessage() {
      const input = document.getElementById("userInput");
      const userMessage = input.value.trim();
      if (!userMessage || !llama) return;

      addMessage(userMessage, "user");
      input.value = "";

      // Показываем индикатор загрузки
      const loadingMsg = addMessage("Думаю...", "ai", true);

      try {
        // Генерируем ответ
        const response = await llama.generate(userMessage, {
          max_tokens: 200,
          temperature: 0.7
        });

        // Удаляем индикатор и выводим ответ
        loadingMsg.remove();
        addMessage(response, "ai");
      } catch (error) {
        loadingMsg.remove();
        addMessage("Ошибка: " + error.message, "ai");
      }
    }

    // Добавление сообщения в чат
    function addMessage(text, role, isLoading = false) {
      const chat = document.getElementById("chat");
      const div = document.createElement("div");
      div.className = role;
      div.textContent = (role === "user" ? "Вы: " : "ИИ: ") + text;

      if (isLoading) {
        div.classList.add("loading");
      }

      chat.appendChild(div);
      chat.scrollTop = chat.scrollHeight; // Прокрутка вниз
      return div;
    }

    // Запуск при загрузке
    window.onload = initAI;
  </script>
</body>
</html>
￼Enter
